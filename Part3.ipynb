{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55854f4-2bad-41af-91aa-65bcd05d908d",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df8f8d-a2d3-47fb-9e3c-a78bbb35d2f9",
   "metadata": {},
   "source": [
    "a) Using the database with 650,000 tweets, create a new table that corresponds to the join of all 3 tables in your database, including records without a geo location. This is the equivalent of a materialized view but since SQLite does not support MVs, we will use CREATE TABLE AS SELECT (instead of CREATE MATERIALIZED VIEW AS SELECT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf490d0-4d64-4a62-87be-d6fe5631cae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Thu May 29 00:00:43 +0000 2014', '471803285746495489', 'There is no wealth but life. ~John Ruskin #wisdomink', '<a href=\"http://www.hootsuite.com\" rel=\"nofollow\">HootSuite</a>', None, None, None, 0, None, 213646047, None, 'Wisdom Ink', 'Wisdom_Ink', 'Wisdom Ink Online Magazine:  Expressing our #joy & #love via our articles. Celebrating diversity & the growth of #consciousness. #zen #meditation #spiritual', 4821, None, None, None)\n",
      "\n",
      "\n",
      "('Thu May 29 00:00:43 +0000 2014', '471803285738106880', 'Mucho la Plop esto, la Plop aquello, pero de los viernes es la fiesta con la gente más linda. \\nEn las otras vienen directo de la frontera.', 'web', None, None, None, 0, None, 38950479, None, 'Yarer Sofier', 'firekites', 'Visite nuestro stand en la planta cuarta. Gran liquidación en revólveres, cuchillos y todos los complementos de la mujer inquieta.', 99, None, None, None)\n",
      "\n",
      "\n",
      "('Thu May 29 00:00:43 +0000 2014', '471803285767462913', 'motive. When a political idea finds its way into such heads,', '<a href=\"http://eto-secret4.ru\" rel=\"nofollow\">eto prosto NEW secret</a>', None, None, None, 0, None, 2443526930, None, 'Vera Luciani', 'rosaxonahag', 'Proud coffee advocate ; studies in  # all about-Devoted alcohol lover .', 261, None, None, None)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "connection = sqlite3.connect('DSC450-Final-650k.db')\n",
    "cursor = connection.cursor()\n",
    "query_3A = '''CREATE TABLE Tweet_Join \n",
    "            AS SELECT Tweet.*,\n",
    "                    User.Name,\n",
    "                    User.SCREEN_NAME,\n",
    "                    User.DESCRIPTION,\n",
    "                    User.FRIENDS_COUNT,\n",
    "                    Geo.Type,\n",
    "                    Geo.LONGITUDE,\n",
    "                    Geo.LATITUDE\n",
    "            FROM Tweet\n",
    "            LEFT JOIN User ON Tweet.USER_ID = User.ID\n",
    "            LEFT JOIN Geo ON Tweet.Geo_ID = Geo.Geo_ID;'''\n",
    "\n",
    "cursor.execute('DROP TABLE IF EXISTS Tweet_Join')\n",
    "cursor.execute(f'{query_3A}')\n",
    "cursor.execute('SELECT * FROM Tweet_Join LIMIT 3')\n",
    "results_3A = cursor.fetchall()\n",
    "for row in results_3A:\n",
    "    print(row)\n",
    "    print(\"\\n\")\n",
    "connection.commit()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0ad77-20a6-491d-a5de-5235dd709a80",
   "metadata": {},
   "source": [
    "b) Export the contents of 1) the Tweet table and 2) your new table from 3-a into a new JSON file (i.e., create your own JSON file with just the keys you extracted). You do not need to replicate the structure of the input and can come up with any reasonable keys for each field stored in JSON structure (e.g., you can have longitude as “longitude” key when the location is available). \n",
    "How do the file sizes compare to the original input file?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7db89f-5001-4167-95b6-33aa6365083d",
   "metadata": {},
   "source": [
    "#### Tweet Table: 11 attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b27c034-dc75-488a-ac2b-bb73f18df5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet JSON file size: 294.24 MB\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "connection = sqlite3.connect('DSC450-Final-650k.db')\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cursor.execute('SELECT * FROM Tweet')\n",
    "tweetResults = cursor.fetchall()\n",
    "\n",
    "tweet_columnNames = [col[0] for col in cursor.description]\n",
    "\n",
    "tweetData = []\n",
    "for row in tweetResults:\n",
    "    tweetDict = {}\n",
    "    for i, value in enumerate(row):\n",
    "        tweetDict[tweet_columnNames[i]] = value\n",
    "    tweetData.append(tweetDict)\n",
    "\n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "with open('Tweet.json', 'w') as tweetFile:\n",
    "    json.dump(tweetData, tweetFile)\n",
    "\n",
    "tweetFile_size_json = os.path.getsize('Tweet.json') / (1024 * 1024)\n",
    "print(f'Tweet JSON file size: {tweetFile_size_json:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845f11b5-e181-4824-8ecd-a19ced3ac0f3",
   "metadata": {},
   "source": [
    "#### Tweet Join table: 18 atributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a0c6fa6-6707-4a89-8e7b-3050b057935f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet_Join JSON file size: 451.45 MB\n"
     ]
    }
   ],
   "source": [
    "connection = sqlite3.connect('DSC450-Final-650k.db')\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cursor.execute('SELECT * FROM Tweet_Join')\n",
    "tweet_joinResults = cursor.fetchall()\n",
    "\n",
    "tweet_join_columnNames = [col[0] for col in cursor.description]\n",
    "\n",
    "tweet_joinData = []\n",
    "for row in tweet_joinResults:\n",
    "    tweet_joinDict = {}\n",
    "    for i, value in enumerate(row):\n",
    "        tweet_joinDict[tweet_join_columnNames[i]] = value\n",
    "    tweet_joinData.append(tweet_joinDict)\n",
    "\n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "with open('Tweet_Join.json', 'w') as tweet_joinFile:\n",
    "    json.dump(tweet_joinData, tweet_joinFile)\n",
    "    tweet_joinFile.write('\\n')\n",
    "\n",
    "tweet_joinFile_size_json = os.path.getsize('tweet_join.json') / (1024 * 1024)\n",
    "print(f'Tweet_Join JSON file size: {tweet_joinFile_size_json:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c49184d-211f-41cd-87e5-6b28263b1377",
   "metadata": {},
   "source": [
    "#### Original URL text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc602123-3a18-4a2a-acbe-54bc6706fe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original textURL File size: 12.95 GB\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "textURL = 'http://dbgroup.cdm.depaul.edu/DSC450/OneDayOfTweets.txt'\n",
    "with urllib.request.urlopen(textURL) as response:\n",
    "    fileSize_bytes = int(response.info().get('Content-Length'))\n",
    "\n",
    "fileSize_gb = fileSize_bytes / (1024 ** 3)\n",
    "print(f\"Original textURL File size: {fileSize_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2ebad-0242-449e-87aa-275feadec5c3",
   "metadata": {},
   "source": [
    "After processing data, the two json files' sizes are Tweet table: 294.24 MB, and Tweet_Join table: 451.45 MB. We have significantly reduced the size from the original textURL file of 12.95 GB. This shows that using JSON file is an effective approach for large data storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7412b11-bc86-406c-9a82-0a06f0145eda",
   "metadata": {},
   "source": [
    "c) Export the contents of 1) the Tweet table and 2) your table from 3-a into a .csv (comma separated value) file. \n",
    "How do the file size compare to the original input file and to the files in 3-b?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee3546b-b756-42b4-b347-775fa95def9a",
   "metadata": {},
   "source": [
    "#### Tweet Table: 11 attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "807c938b-82b0-4e29-bea6-41071452a8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet CSV file size: 145.85 MB\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "connection = sqlite3.connect('DSC450-Final-650k.db')\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM Tweet\")\n",
    "tweetResults = cursor.fetchall()\n",
    "\n",
    "columnNames = [col[0] for col in cursor.description]\n",
    "\n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "with open('Tweet.csv', 'w', newline='', encoding='utf-8-sig') as tweet_csvFile:\n",
    "    tweet_csvWriter = csv.writer(tweet_csvFile)\n",
    "    tweet_csvWriter.writerow(columnNames)\n",
    "    tweet_csvWriter.writerows(tweetResults)\n",
    "        \n",
    "tweetFile_size_csv = os.path.getsize('Tweet.csv') / (1024 * 1024)\n",
    "print(f'Tweet CSV file size: {tweetFile_size_csv:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f6083-7889-458a-9de9-b4832f7568eb",
   "metadata": {},
   "source": [
    "#### Tweet Join table: 18 atributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf65bb75-4b0f-43e3-b729-9b1a1798f01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet_Join CSV file size: 212.04 MB\n"
     ]
    }
   ],
   "source": [
    "connection = sqlite3.connect('DSC450-Final-650k.db')\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM Tweet_Join\")\n",
    "tweet_joinResults = cursor.fetchall()\n",
    "\n",
    "columnNames = [col[0] for col in cursor.description]\n",
    "\n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "with open('Tweet_Join.csv', 'w', newline='', encoding='utf-8-sig') as tweet_join_csvFile:\n",
    "    tweet_join_csvWriter = csv.writer(tweet_join_csvFile)\n",
    "    tweet_join_csvWriter.writerow(columnNames)\n",
    "    tweet_join_csvWriter.writerows(tweet_joinResults)\n",
    "\n",
    "tweet_joinFile_size_csv = os.path.getsize('Tweet_Join.csv') / (1024 * 1024)\n",
    "print(f'Tweet_Join CSV file size: {tweet_joinFile_size_csv:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a1a00b-36ad-4477-938f-9ed5e677d63f",
   "metadata": {},
   "source": [
    "After processing data, the two csv files' sizes are Tweet table: 145.85 MB, and Tweet_Join table: 212.04 MB. We have significantly reduced the size from the original textURL file of 12.95 GB. CSV files size are also smaller than JSON files. This shows that using CSV file is the most effective approach for large data storage in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7bd6a2-2b14-4576-9473-2a1e4aac4ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
